{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hipothesis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "42j0U8RQNmvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6cd89e94-87d7-4da2-aa66-433297d6c20d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C3JfvqDfNpwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5744d021-5964-439e-efff-ec4bd8487550"
      },
      "cell_type": "code",
      "source": [
        "!dir drive/My\\ Drive/model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BEST_checkpoint_synthesisweb_1_cap_per_img_1_min_word_freq.pth.tar\n",
            "checkpoint_synthesisweb_1_cap_per_img_1_min_word_freq.pth.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XWT4klKYNsYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!touch drive/My\\ Drive/model/test.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdzO7JPlIK-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f350a846-8d67-4e76-fa90-cfb3ee1c745d"
      },
      "cell_type": "code",
      "source": [
        "!rm data.zip\n",
        "!curl -L -o data.zip https://www.dropbox.com/s/n2i4e4yqltrftdv/data.zip?dl=1\n",
        "!rm -rf data\n",
        "!unzip -qq data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  296M  100  296M    0     0  32.8M      0  0:00:09  0:00:09 --:--:-- 36.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tc0WEjlvIPIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "efed4612-2819-45fb-9b5f-afd23cf09caa"
      },
      "cell_type": "code",
      "source": [
        "# remove filenya dulu biar ga duplicate\n",
        "!rm bootstrap.vocab\n",
        "# download folder bahan training\n",
        "!curl -O https://gist.githubusercontent.com/laptopmutia/71fb3aad710e1fd6b6944973b434dd80/raw/0df8d43dac8ff6c475b49799d635ee8f5362b6e0/bootstrap.vocab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   110  100   110    0     0    110      0  0:00:01 --:--:--  0:00:01   625\r100   110  100   110    0     0    110      0  0:00:01 --:--:--  0:00:01   621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mLX1JhS_aeDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c77b7710-aa7e-45f7-dc4a-1814bd64be48"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x2g1DwdHafZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from scipy.misc import imread, imresize\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "from torch.utils.data import Dataset\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import time \n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4SfxtfJafeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_input_files(dataset, data_dir_path, captions_per_image, min_word_freq, output_folder,\n",
        "                       vocab_path, max_len=120):\n",
        "        # Read image paths and captions for each image\n",
        "        train_image_paths = []\n",
        "        train_image_captions = []\n",
        "        val_image_paths = []\n",
        "        val_image_captions = []\n",
        "        test_image_paths = []\n",
        "        test_image_captions = []\n",
        "\n",
        "        data_train_path = os.listdir(os.path.join(data_dir_path, 'train/'))\n",
        "        data_test_path = os.listdir(os.path.join(data_dir_path, 'test/'))\n",
        "        data_val_path = os.listdir(os.path.join(data_dir_path, 'val/'))\n",
        "\n",
        "        for data_path, folder, image_paths, caption_paths in [\n",
        "                (data_train_path, 'train/', train_image_paths, train_image_captions),\n",
        "                (data_test_path, 'test/', test_image_paths, test_image_captions),\n",
        "                (data_val_path, 'val/', val_image_paths, val_image_captions)]:\n",
        "\n",
        "            full_path = os.path.join(data_dir_path, folder)\n",
        "            for filename in data_path:\n",
        "                if filename[-3:] == 'png':\n",
        "                    image_paths.append(os.path.join(full_path, filename))\n",
        "                elif filename[-3:] == 'gui':\n",
        "                    caption_paths.append(os.path.join(full_path, filename))\n",
        "\n",
        "        #sort\n",
        "        train_image_paths.sort()\n",
        "        train_image_captions.sort()\n",
        "        val_image_paths.sort()\n",
        "        val_image_captions.sort()\n",
        "        test_image_paths.sort()\n",
        "        test_image_captions.sort()\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(train_image_paths) == len(train_image_captions)\n",
        "        assert len(val_image_paths) == len(val_image_captions)\n",
        "        assert len(test_image_paths) == len(test_image_captions)\n",
        "\n",
        "        #create word map\n",
        "        vocab_file = open(vocab_path, 'r')\n",
        "        words = vocab_file.read()\n",
        "        vocab_file.close()\n",
        "        words = words.split()\n",
        "        word_map = {k: v+1 for v, k in enumerate(words)}\n",
        "        word_map['<unk>'] = len(word_map) + 1\n",
        "        word_map['<start>'] = len(word_map) + 1\n",
        "        word_map['<end>'] = len(word_map) + 1\n",
        "        word_map['<pad>'] = 0\n",
        "\n",
        "        # Create a base/root name for all output files\n",
        "        base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
        "\n",
        "        # Save word map to a JSON\n",
        "        with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
        "            json.dump(word_map, j)\n",
        "\n",
        "        # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n",
        "        seed(123)\n",
        "        for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
        "                                        (val_image_paths, val_image_captions, 'VAL'),\n",
        "                                        (test_image_paths, test_image_captions, 'TEST')]:\n",
        "\n",
        "            with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
        "                # Make a note of the number of captions we are sampling per image\n",
        "                h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "                # Create dataset inside HDF5 file to store images\n",
        "                images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "                print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
        "\n",
        "                enc_captions = []\n",
        "                caplens = []\n",
        "\n",
        "                for i, path in enumerate(tqdm(impaths)):\n",
        "\n",
        "                    # Read images\n",
        "                    img = imread(impaths[i], mode='RGB')\n",
        "                    if len(img.shape) == 2:\n",
        "                        img = img[:, :, np.newaxis]\n",
        "                        img = np.concatenate([img, img, img], axis=2)\n",
        "                    img = imresize(img, (256, 256))\n",
        "                    #ngerubah shape biar channel didepan resolusi dibelakang\n",
        "                    img = img.transpose(2, 0, 1)\n",
        "                    assert img.shape == (3, 256, 256)\n",
        "                    assert np.max(img) <= 255\n",
        "\n",
        "                    # Save image to HDF5 file\n",
        "                    images[i] = img\n",
        "\n",
        "                    # read captions file\n",
        "                    caption_file = open(imcaps[i], 'r')\n",
        "                    c = caption_file.read()\n",
        "                    caption_file.close()\n",
        "                    c = c.split()\n",
        "                    # Encode captions\n",
        "                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n",
        "                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n",
        "\n",
        "                    # Find caption lengths\n",
        "                    c_len = len(c) + 2\n",
        "\n",
        "                    enc_captions.append(enc_c)\n",
        "                    caplens.append(c_len)\n",
        "\n",
        "                # Sanity check\n",
        "                assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
        "\n",
        "                # Save encoded captions and their lengths to JSON files\n",
        "                with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
        "                    json.dump(enc_captions, j)\n",
        "\n",
        "                with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
        "                    json.dump(caplens, j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WPa-qPtIafg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWypfi7Qafjf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    :param data_name: base name of processed dataset\n",
        "    :param epoch: epoch number\n",
        "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param bleu4: validation BLEU-4 score for this epoch\n",
        "    :param is_best: is this checkpoint the best so far?\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    foldernya = '/content/drive/My Drive/model/'\n",
        "   # foldernya = '/content/'\n",
        "    torch.save(state, foldernya+filename)\n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    if is_best:\n",
        "        torch.save(state, foldernya+ 'BEST_' + filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBVlOp0GafmJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NSFpHu87afod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MP1VNrdVafrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCyvRcUFafy7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf ./output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhs5267-afw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir ./output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SApPRQgJaftt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "1c402340-d4af-4947-8b84-5ba8daeb55a6"
      },
      "cell_type": "code",
      "source": [
        "create_input_files(dataset='synthesisweb',\n",
        "                   data_dir_path = './data/',\n",
        "                   vocab_path = './bootstrap.vocab',\n",
        "                   captions_per_image=1,\n",
        "                   min_word_freq=1,\n",
        "                   output_folder='./output/',\n",
        "                   max_len=120)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1400 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if issubdtype(ts, int):\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  elif issubdtype(type(size), float):\n",
            "  0%|          | 1/1400 [00:00<02:55,  7.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading TRAIN images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:44<00:00, 13.46it/s]\n",
            "  1%|          | 2/175 [00:00<00:13, 13.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading VAL images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 175/175 [00:12<00:00, 13.55it/s]\n",
            "  1%|          | 2/175 [00:00<00:12, 13.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading TEST images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 175/175 [00:13<00:00, 13.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Np3WMoseaf1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data parameters\n",
        "data_folder = './output'  # folder with data files saved by create_input_files.py\n",
        "data_name = 'synthesisweb_1_cap_per_img_1_min_word_freq'  # base name shared by data files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnI_uCtaaf62",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdoWJK0XagAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "start_epoch = 0\n",
        "epochs = 1000  # number of epochs to train for (if early stopping is not triggered)\n",
        "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 4\n",
        "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "grad_clip = 5.  # clip gradients at an absolute value of\n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
        "best_bleu4 = 0.  # BLEU-4 score right now\n",
        "print_freq_train = 70  # print training/validation stats every __ batches\n",
        "print_freq_val = 22\n",
        "fine_tune_encoder = False  # fine-tune encoder?\n",
        "# checkpoint = None\n",
        "checkpoint = '/content/drive/My Drive/model/checkpoint_synthesisweb_1_cap_per_img_1_min_word_freq.pth.tar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eim-gc13agDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, data_name, split, transform=None):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param data_name: base name of processed datasets\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        :param transform: image transform pipeline\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']\n",
        "\n",
        "        # Load encoded captions (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
        "            self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
        "            self.caplens = json.load(j)\n",
        "\n",
        "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'TRAIN':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HS_ARDCcagGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r90PDz8LagLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJ8gWfelavfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNVbzSmAagIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to GPU, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        # pack_padded_sequence is an easy trick to do this\n",
        "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop.\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq_train == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          batch_time=batch_time,\n",
        "                                                                          data_time=data_time, loss=losses,\n",
        "                                                                          top5=top5accs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55Fd1dwmaf95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "\n",
        "        # Move to device, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        if encoder is not None:\n",
        "            imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        # pack_padded_sequence is an easy trick to do this\n",
        "        scores_copy = scores.clone()\n",
        "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % print_freq_val == 0:\n",
        "            print('Validation: [{0}/{1}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                            loss=losses, top5=top5accs))\n",
        "\n",
        "        # Store references (true captions), and hypothesis (prediction) for each image\n",
        "        # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "        # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "        # References\n",
        "        allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "        for j in range(allcaps.shape[0]):\n",
        "            img_caps = allcaps[j].tolist()\n",
        "            img_captions = list(\n",
        "                map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                    img_caps))  # remove <start> and pads\n",
        "            references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        _, preds = torch.max(scores_copy, dim=2)\n",
        "        preds = preds.tolist()\n",
        "        temp_preds = list()\n",
        "        for j, p in enumerate(preds):\n",
        "            temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "        preds = temp_preds\n",
        "        hypotheses.extend(preds)\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "    bleu4 = corpus_bleu(references, hypotheses, emulate_multibleu=True)\n",
        "\n",
        "    print(\n",
        "        '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "            loss=losses,\n",
        "            top5=top5accs,\n",
        "            bleu=bleu4))\n",
        "\n",
        "    return bleu4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i3BDRN_0af4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7160
        },
        "outputId": "758985f9-ef8b-4658-a599-0b7de509a43d"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training and validation.\n",
        "\"\"\"\n",
        "\n",
        "global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n",
        "\n",
        "# Read word map\n",
        "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "\n",
        "# Initialize / load checkpoint\n",
        "if checkpoint is None:\n",
        "    decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                   embed_dim=emb_dim,\n",
        "                                   decoder_dim=decoder_dim,\n",
        "                                   vocab_size=len(word_map),\n",
        "                                   dropout=dropout)\n",
        "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                         lr=decoder_lr)\n",
        "    encoder = Encoder()\n",
        "    encoder.fine_tune(fine_tune_encoder)\n",
        "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                         lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "else:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "    best_bleu4 = checkpoint['bleu-4']\n",
        "    decoder = checkpoint['decoder']\n",
        "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
        "    encoder = checkpoint['encoder']\n",
        "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
        "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr)\n",
        "\n",
        "# Move to GPU, if available\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Custom dataloaders\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "#     print(\"lontong\")\n",
        "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "    if epochs_since_improvement == 200:\n",
        "        break\n",
        "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "        if fine_tune_encoder:\n",
        "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
        "\n",
        "    # One epoch's training\n",
        "    train(train_loader=train_loader,\n",
        "          encoder=encoder,\n",
        "          decoder=decoder,\n",
        "          criterion=criterion,\n",
        "          encoder_optimizer=encoder_optimizer,\n",
        "          decoder_optimizer=decoder_optimizer,\n",
        "          epoch=epoch)\n",
        "\n",
        "    # One epoch's validation\n",
        "    recent_bleu4 = validate(val_loader=val_loader,\n",
        "                            encoder=encoder,\n",
        "                            decoder=decoder,\n",
        "                            criterion=criterion)\n",
        "\n",
        "    # Check if there was an improvement\n",
        "    is_best = recent_bleu4 > best_bleu4\n",
        "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
        "                    decoder_optimizer, recent_bleu4, is_best)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [167][0/350]\tBatch Time 2.492 (2.492)\tData Load Time 0.067 (0.067)\tLoss 0.4207 (0.4207)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [167][70/350]\tBatch Time 0.614 (0.657)\tData Load Time 0.000 (0.001)\tLoss 0.4030 (0.4231)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [167][140/350]\tBatch Time 0.618 (0.647)\tData Load Time 0.000 (0.001)\tLoss 0.3909 (0.4231)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [167][210/350]\tBatch Time 0.607 (0.637)\tData Load Time 0.000 (0.000)\tLoss 0.4448 (0.4283)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [167][280/350]\tBatch Time 0.624 (0.634)\tData Load Time 0.000 (0.000)\tLoss 0.4970 (0.4271)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.357 (0.357)\tLoss 0.3543 (0.3543)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.246 (0.264)\tLoss 0.4733 (0.4238)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9984\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DecoderWithAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [168][0/350]\tBatch Time 0.773 (0.773)\tData Load Time 0.059 (0.059)\tLoss 0.5617 (0.5617)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [168][70/350]\tBatch Time 0.614 (0.654)\tData Load Time 0.000 (0.001)\tLoss 0.4443 (0.4259)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [168][140/350]\tBatch Time 0.605 (0.649)\tData Load Time 0.000 (0.001)\tLoss 0.3877 (0.4305)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [168][210/350]\tBatch Time 0.625 (0.642)\tData Load Time 0.000 (0.000)\tLoss 0.3659 (0.4274)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [168][280/350]\tBatch Time 0.625 (0.637)\tData Load Time 0.000 (0.000)\tLoss 0.4725 (0.4257)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.345 (0.345)\tLoss 0.3846 (0.3846)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.243 (0.269)\tLoss 0.4511 (0.4064)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.420, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9986\n",
            "\n",
            "Epoch: [169][0/350]\tBatch Time 0.764 (0.764)\tData Load Time 0.058 (0.058)\tLoss 0.3977 (0.3977)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [169][70/350]\tBatch Time 0.629 (0.653)\tData Load Time 0.000 (0.001)\tLoss 0.3814 (0.4234)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [169][140/350]\tBatch Time 0.628 (0.641)\tData Load Time 0.000 (0.001)\tLoss 0.4329 (0.4256)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [169][210/350]\tBatch Time 0.625 (0.636)\tData Load Time 0.000 (0.000)\tLoss 0.3751 (0.4262)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [169][280/350]\tBatch Time 0.870 (0.634)\tData Load Time 0.000 (0.000)\tLoss 0.3294 (0.4258)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.323 (0.323)\tLoss 0.4990 (0.4990)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.260 (0.264)\tLoss 0.3783 (0.4203)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.420, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "Epoch: [170][0/350]\tBatch Time 0.813 (0.813)\tData Load Time 0.056 (0.056)\tLoss 0.4425 (0.4425)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [170][70/350]\tBatch Time 0.630 (0.660)\tData Load Time 0.000 (0.001)\tLoss 0.4386 (0.4148)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [170][140/350]\tBatch Time 0.636 (0.644)\tData Load Time 0.000 (0.001)\tLoss 0.3657 (0.4261)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [170][210/350]\tBatch Time 0.613 (0.638)\tData Load Time 0.000 (0.000)\tLoss 0.4061 (0.4268)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [170][280/350]\tBatch Time 0.627 (0.635)\tData Load Time 0.000 (0.000)\tLoss 0.5290 (0.4259)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.362 (0.362)\tLoss 0.3689 (0.3689)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.265 (0.261)\tLoss 0.3630 (0.4341)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.421, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9981\n",
            "\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n",
            "Epoch: [171][0/350]\tBatch Time 0.738 (0.738)\tData Load Time 0.056 (0.056)\tLoss 0.5836 (0.5836)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [171][70/350]\tBatch Time 0.604 (0.654)\tData Load Time 0.000 (0.001)\tLoss 0.5572 (0.4291)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [171][140/350]\tBatch Time 0.611 (0.641)\tData Load Time 0.000 (0.001)\tLoss 0.4230 (0.4345)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [171][210/350]\tBatch Time 0.590 (0.640)\tData Load Time 0.000 (0.000)\tLoss 0.5623 (0.4286)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [171][280/350]\tBatch Time 0.616 (0.639)\tData Load Time 0.000 (0.000)\tLoss 0.4094 (0.4269)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.317 (0.317)\tLoss 0.6048 (0.6048)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.245 (0.265)\tLoss 0.4056 (0.4220)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.421, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9984\n",
            "\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n",
            "Epoch: [172][0/350]\tBatch Time 0.705 (0.705)\tData Load Time 0.056 (0.056)\tLoss 0.3877 (0.3877)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [172][70/350]\tBatch Time 0.617 (0.644)\tData Load Time 0.000 (0.001)\tLoss 0.4553 (0.4251)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [172][140/350]\tBatch Time 0.606 (0.640)\tData Load Time 0.000 (0.001)\tLoss 0.4098 (0.4269)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [172][210/350]\tBatch Time 0.613 (0.637)\tData Load Time 0.000 (0.000)\tLoss 0.3876 (0.4252)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [172][280/350]\tBatch Time 0.634 (0.639)\tData Load Time 0.000 (0.000)\tLoss 0.5513 (0.4241)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.321 (0.321)\tLoss 0.4126 (0.4126)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.253 (0.262)\tLoss 0.4411 (0.4333)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9981\n",
            "\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n",
            "Epoch: [173][0/350]\tBatch Time 0.747 (0.747)\tData Load Time 0.060 (0.060)\tLoss 0.4896 (0.4896)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [173][70/350]\tBatch Time 0.638 (0.648)\tData Load Time 0.000 (0.001)\tLoss 0.3909 (0.4286)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [173][140/350]\tBatch Time 0.681 (0.646)\tData Load Time 0.000 (0.001)\tLoss 0.4125 (0.4303)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [173][210/350]\tBatch Time 0.623 (0.646)\tData Load Time 0.000 (0.000)\tLoss 0.3790 (0.4272)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [173][280/350]\tBatch Time 0.606 (0.641)\tData Load Time 0.000 (0.000)\tLoss 0.4120 (0.4279)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.319 (0.319)\tLoss 0.3814 (0.3814)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.251 (0.265)\tLoss 0.3815 (0.4230)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.422, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 5\n",
            "\n",
            "Epoch: [174][0/350]\tBatch Time 0.749 (0.749)\tData Load Time 0.061 (0.061)\tLoss 0.3719 (0.3719)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [174][70/350]\tBatch Time 0.625 (0.634)\tData Load Time 0.000 (0.001)\tLoss 0.4389 (0.4399)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [174][140/350]\tBatch Time 0.577 (0.631)\tData Load Time 0.000 (0.001)\tLoss 0.4345 (0.4281)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [174][210/350]\tBatch Time 0.591 (0.629)\tData Load Time 0.000 (0.000)\tLoss 0.4083 (0.4267)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [174][280/350]\tBatch Time 0.619 (0.629)\tData Load Time 0.000 (0.000)\tLoss 0.4481 (0.4256)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.328 (0.328)\tLoss 0.4816 (0.4816)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.284 (0.266)\tLoss 0.3713 (0.4304)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.422, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9987\n",
            "\n",
            "Epoch: [175][0/350]\tBatch Time 0.881 (0.881)\tData Load Time 0.061 (0.061)\tLoss 0.4222 (0.4222)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [175][70/350]\tBatch Time 0.614 (0.640)\tData Load Time 0.000 (0.001)\tLoss 0.5589 (0.4251)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [175][140/350]\tBatch Time 0.603 (0.632)\tData Load Time 0.000 (0.001)\tLoss 0.4526 (0.4232)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [175][210/350]\tBatch Time 0.622 (0.632)\tData Load Time 0.000 (0.000)\tLoss 0.4459 (0.4274)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [175][280/350]\tBatch Time 0.625 (0.630)\tData Load Time 0.000 (0.000)\tLoss 0.3848 (0.4269)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.322 (0.322)\tLoss 0.3987 (0.3987)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.246 (0.260)\tLoss 0.4911 (0.4369)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9986\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "Epoch: [176][0/350]\tBatch Time 0.731 (0.731)\tData Load Time 0.057 (0.057)\tLoss 0.4174 (0.4174)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [176][70/350]\tBatch Time 0.612 (0.640)\tData Load Time 0.000 (0.001)\tLoss 0.4899 (0.4292)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [176][140/350]\tBatch Time 0.593 (0.633)\tData Load Time 0.000 (0.001)\tLoss 0.3907 (0.4312)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [176][210/350]\tBatch Time 0.629 (0.632)\tData Load Time 0.000 (0.000)\tLoss 0.3814 (0.4277)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [176][280/350]\tBatch Time 0.743 (0.633)\tData Load Time 0.000 (0.000)\tLoss 0.3798 (0.4260)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.320 (0.320)\tLoss 0.4764 (0.4764)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.254 (0.265)\tLoss 0.5236 (0.4339)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9981\n",
            "\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n",
            "Epoch: [177][0/350]\tBatch Time 0.704 (0.704)\tData Load Time 0.058 (0.058)\tLoss 0.3845 (0.3845)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [177][70/350]\tBatch Time 0.841 (0.629)\tData Load Time 0.000 (0.001)\tLoss 0.3378 (0.4327)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [177][140/350]\tBatch Time 0.611 (0.621)\tData Load Time 0.000 (0.001)\tLoss 0.3657 (0.4277)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [177][210/350]\tBatch Time 0.610 (0.625)\tData Load Time 0.000 (0.000)\tLoss 0.4098 (0.4263)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [177][280/350]\tBatch Time 0.639 (0.627)\tData Load Time 0.000 (0.000)\tLoss 0.4752 (0.4253)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.335 (0.335)\tLoss 0.4398 (0.4398)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.252 (0.267)\tLoss 0.3751 (0.4285)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n",
            "Epoch: [178][0/350]\tBatch Time 0.728 (0.728)\tData Load Time 0.059 (0.059)\tLoss 0.4040 (0.4040)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [178][70/350]\tBatch Time 0.693 (0.633)\tData Load Time 0.000 (0.001)\tLoss 0.3482 (0.4263)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [178][140/350]\tBatch Time 0.598 (0.630)\tData Load Time 0.000 (0.001)\tLoss 0.4323 (0.4265)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [178][210/350]\tBatch Time 0.593 (0.628)\tData Load Time 0.000 (0.000)\tLoss 0.4022 (0.4283)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [178][280/350]\tBatch Time 0.627 (0.627)\tData Load Time 0.000 (0.000)\tLoss 0.4632 (0.4269)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.336 (0.336)\tLoss 0.3964 (0.3964)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.258 (0.268)\tLoss 0.4070 (0.4268)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9987\n",
            "\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n",
            "Epoch: [179][0/350]\tBatch Time 0.862 (0.862)\tData Load Time 0.060 (0.060)\tLoss 0.3397 (0.3397)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [179][70/350]\tBatch Time 0.648 (0.648)\tData Load Time 0.000 (0.001)\tLoss 0.3751 (0.4277)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [179][140/350]\tBatch Time 0.636 (0.642)\tData Load Time 0.000 (0.001)\tLoss 0.3978 (0.4256)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [179][210/350]\tBatch Time 0.627 (0.638)\tData Load Time 0.000 (0.000)\tLoss 0.4876 (0.4262)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [179][280/350]\tBatch Time 0.646 (0.637)\tData Load Time 0.000 (0.000)\tLoss 0.3674 (0.4263)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.330 (0.330)\tLoss 0.4139 (0.4139)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.255 (0.259)\tLoss 0.4581 (0.4378)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9987\n",
            "\n",
            "\n",
            "Epochs since last improvement: 5\n",
            "\n",
            "Epoch: [180][0/350]\tBatch Time 0.719 (0.719)\tData Load Time 0.056 (0.056)\tLoss 0.4745 (0.4745)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [180][70/350]\tBatch Time 0.631 (0.638)\tData Load Time 0.000 (0.001)\tLoss 0.4603 (0.4338)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [180][140/350]\tBatch Time 0.621 (0.635)\tData Load Time 0.000 (0.001)\tLoss 0.3845 (0.4240)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [180][210/350]\tBatch Time 0.624 (0.632)\tData Load Time 0.000 (0.000)\tLoss 0.3688 (0.4256)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [180][280/350]\tBatch Time 0.640 (0.631)\tData Load Time 0.000 (0.000)\tLoss 0.4891 (0.4266)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.374 (0.374)\tLoss 0.4129 (0.4129)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.262 (0.261)\tLoss 0.5580 (0.4225)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.422, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9989\n",
            "\n",
            "Epoch: [181][0/350]\tBatch Time 0.744 (0.744)\tData Load Time 0.064 (0.064)\tLoss 0.4302 (0.4302)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [181][70/350]\tBatch Time 0.614 (0.640)\tData Load Time 0.000 (0.001)\tLoss 0.3719 (0.4242)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [181][140/350]\tBatch Time 0.585 (0.628)\tData Load Time 0.000 (0.001)\tLoss 0.4428 (0.4214)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [181][210/350]\tBatch Time 0.619 (0.624)\tData Load Time 0.000 (0.000)\tLoss 0.5226 (0.4206)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [181][280/350]\tBatch Time 0.598 (0.622)\tData Load Time 0.000 (0.000)\tLoss 0.4294 (0.4229)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.331 (0.331)\tLoss 0.3723 (0.3723)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.249 (0.263)\tLoss 0.4277 (0.4342)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9987\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "Epoch: [182][0/350]\tBatch Time 0.732 (0.732)\tData Load Time 0.060 (0.060)\tLoss 0.3782 (0.3782)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [182][70/350]\tBatch Time 0.725 (0.629)\tData Load Time 0.000 (0.001)\tLoss 0.4346 (0.4271)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [182][140/350]\tBatch Time 0.578 (0.617)\tData Load Time 0.000 (0.001)\tLoss 0.3908 (0.4265)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [182][210/350]\tBatch Time 0.599 (0.619)\tData Load Time 0.000 (0.000)\tLoss 0.4408 (0.4246)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [182][280/350]\tBatch Time 0.584 (0.617)\tData Load Time 0.000 (0.000)\tLoss 0.4270 (0.4257)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.316 (0.316)\tLoss 0.3719 (0.3719)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.243 (0.258)\tLoss 0.4711 (0.4262)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.424, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9979\n",
            "\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n",
            "Epoch: [183][0/350]\tBatch Time 0.752 (0.752)\tData Load Time 0.068 (0.068)\tLoss 0.4513 (0.4513)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [183][70/350]\tBatch Time 0.600 (0.632)\tData Load Time 0.000 (0.001)\tLoss 0.5520 (0.4213)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [183][140/350]\tBatch Time 0.606 (0.626)\tData Load Time 0.000 (0.001)\tLoss 0.5125 (0.4255)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [183][210/350]\tBatch Time 0.774 (0.623)\tData Load Time 0.000 (0.000)\tLoss 0.3648 (0.4266)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [183][280/350]\tBatch Time 0.614 (0.620)\tData Load Time 0.000 (0.000)\tLoss 0.4844 (0.4279)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.326 (0.326)\tLoss 0.6462 (0.6462)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.246 (0.265)\tLoss 0.4988 (0.4161)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.419, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9986\n",
            "\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n",
            "Epoch: [184][0/350]\tBatch Time 0.720 (0.720)\tData Load Time 0.061 (0.061)\tLoss 0.4625 (0.4625)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [184][70/350]\tBatch Time 0.581 (0.633)\tData Load Time 0.000 (0.001)\tLoss 0.4350 (0.4282)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [184][140/350]\tBatch Time 0.583 (0.626)\tData Load Time 0.000 (0.001)\tLoss 0.3940 (0.4268)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [184][210/350]\tBatch Time 0.630 (0.625)\tData Load Time 0.000 (0.000)\tLoss 0.3877 (0.4244)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [184][280/350]\tBatch Time 0.588 (0.623)\tData Load Time 0.000 (0.000)\tLoss 0.5181 (0.4262)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.317 (0.317)\tLoss 0.5086 (0.5086)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.255 (0.262)\tLoss 0.4786 (0.4343)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.425, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n",
            "Epoch: [185][0/350]\tBatch Time 0.966 (0.966)\tData Load Time 0.057 (0.057)\tLoss 0.4204 (0.4204)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [185][70/350]\tBatch Time 0.598 (0.644)\tData Load Time 0.000 (0.001)\tLoss 0.5197 (0.4288)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [185][140/350]\tBatch Time 0.596 (0.632)\tData Load Time 0.000 (0.001)\tLoss 0.3782 (0.4254)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [185][210/350]\tBatch Time 0.606 (0.623)\tData Load Time 0.000 (0.000)\tLoss 0.4906 (0.4287)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [185][280/350]\tBatch Time 0.609 (0.620)\tData Load Time 0.000 (0.000)\tLoss 0.3927 (0.4277)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.323 (0.323)\tLoss 0.4076 (0.4076)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.251 (0.260)\tLoss 0.4602 (0.4341)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.425, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9977\n",
            "\n",
            "\n",
            "Epochs since last improvement: 5\n",
            "\n",
            "Epoch: [186][0/350]\tBatch Time 0.834 (0.834)\tData Load Time 0.058 (0.058)\tLoss 0.4066 (0.4066)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [186][70/350]\tBatch Time 0.605 (0.637)\tData Load Time 0.000 (0.001)\tLoss 0.3719 (0.4376)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [186][140/350]\tBatch Time 0.597 (0.627)\tData Load Time 0.000 (0.001)\tLoss 0.4544 (0.4290)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [186][210/350]\tBatch Time 0.638 (0.627)\tData Load Time 0.000 (0.000)\tLoss 0.3612 (0.4270)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [186][280/350]\tBatch Time 0.614 (0.621)\tData Load Time 0.000 (0.000)\tLoss 0.3846 (0.4278)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.318 (0.318)\tLoss 0.3815 (0.3815)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.254 (0.259)\tLoss 0.5068 (0.4334)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.998\n",
            "\n",
            "\n",
            "Epochs since last improvement: 6\n",
            "\n",
            "Epoch: [187][0/350]\tBatch Time 0.749 (0.749)\tData Load Time 0.058 (0.058)\tLoss 0.3813 (0.3813)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [187][70/350]\tBatch Time 0.827 (0.638)\tData Load Time 0.000 (0.001)\tLoss 0.3375 (0.4291)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [187][140/350]\tBatch Time 0.620 (0.627)\tData Load Time 0.000 (0.001)\tLoss 0.4702 (0.4261)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [187][210/350]\tBatch Time 0.585 (0.622)\tData Load Time 0.000 (0.000)\tLoss 0.5185 (0.4274)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [187][280/350]\tBatch Time 0.570 (0.622)\tData Load Time 0.000 (0.000)\tLoss 0.5670 (0.4268)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.322 (0.322)\tLoss 0.4049 (0.4049)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.297 (0.265)\tLoss 0.3507 (0.4158)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.426, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9974\n",
            "\n",
            "\n",
            "Epochs since last improvement: 7\n",
            "\n",
            "Epoch: [188][0/350]\tBatch Time 0.682 (0.682)\tData Load Time 0.057 (0.057)\tLoss 0.5733 (0.5733)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [188][70/350]\tBatch Time 0.834 (0.623)\tData Load Time 0.000 (0.001)\tLoss 0.4088 (0.4282)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [188][140/350]\tBatch Time 0.688 (0.623)\tData Load Time 0.000 (0.001)\tLoss 0.3636 (0.4281)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [188][210/350]\tBatch Time 0.583 (0.621)\tData Load Time 0.000 (0.000)\tLoss 0.4764 (0.4304)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [188][280/350]\tBatch Time 0.744 (0.619)\tData Load Time 0.000 (0.000)\tLoss 0.3869 (0.4292)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.319 (0.319)\tLoss 0.5078 (0.5078)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.250 (0.261)\tLoss 0.3814 (0.4189)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.421, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9988\n",
            "\n",
            "\n",
            "Epochs since last improvement: 8\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000009\n",
            "\n",
            "Epoch: [189][0/350]\tBatch Time 0.728 (0.728)\tData Load Time 0.062 (0.062)\tLoss 0.3783 (0.3783)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [189][70/350]\tBatch Time 0.617 (0.636)\tData Load Time 0.000 (0.001)\tLoss 0.4449 (0.4203)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [189][140/350]\tBatch Time 0.629 (0.631)\tData Load Time 0.000 (0.001)\tLoss 0.3674 (0.4240)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [189][210/350]\tBatch Time 0.615 (0.626)\tData Load Time 0.000 (0.000)\tLoss 0.3751 (0.4249)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [189][280/350]\tBatch Time 0.611 (0.621)\tData Load Time 0.000 (0.000)\tLoss 0.4647 (0.4257)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.321 (0.321)\tLoss 0.4352 (0.4352)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.257 (0.261)\tLoss 0.3768 (0.4264)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9977\n",
            "\n",
            "\n",
            "Epochs since last improvement: 9\n",
            "\n",
            "Epoch: [190][0/350]\tBatch Time 0.744 (0.744)\tData Load Time 0.058 (0.058)\tLoss 0.4294 (0.4294)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [190][70/350]\tBatch Time 0.672 (0.625)\tData Load Time 0.000 (0.001)\tLoss 0.4551 (0.4292)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [190][140/350]\tBatch Time 0.599 (0.619)\tData Load Time 0.000 (0.001)\tLoss 0.3783 (0.4292)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [190][210/350]\tBatch Time 0.639 (0.620)\tData Load Time 0.000 (0.000)\tLoss 0.3784 (0.4261)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [190][280/350]\tBatch Time 0.610 (0.619)\tData Load Time 0.000 (0.000)\tLoss 0.3814 (0.4270)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.320 (0.320)\tLoss 0.3814 (0.3814)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.251 (0.262)\tLoss 0.5507 (0.4294)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.421, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 10\n",
            "\n",
            "Epoch: [191][0/350]\tBatch Time 0.738 (0.738)\tData Load Time 0.057 (0.057)\tLoss 0.4966 (0.4966)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [191][70/350]\tBatch Time 0.614 (0.621)\tData Load Time 0.000 (0.001)\tLoss 0.3814 (0.4248)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [191][140/350]\tBatch Time 0.612 (0.624)\tData Load Time 0.000 (0.001)\tLoss 0.4637 (0.4267)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [191][210/350]\tBatch Time 0.671 (0.621)\tData Load Time 0.000 (0.000)\tLoss 0.4514 (0.4280)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [191][280/350]\tBatch Time 0.621 (0.620)\tData Load Time 0.000 (0.000)\tLoss 0.4034 (0.4275)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.312 (0.312)\tLoss 0.3876 (0.3876)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.253 (0.252)\tLoss 0.4827 (0.4183)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.419, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9986\n",
            "\n",
            "\n",
            "Epochs since last improvement: 11\n",
            "\n",
            "Epoch: [192][0/350]\tBatch Time 0.763 (0.763)\tData Load Time 0.059 (0.059)\tLoss 0.4389 (0.4389)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [192][70/350]\tBatch Time 0.619 (0.625)\tData Load Time 0.000 (0.001)\tLoss 0.3782 (0.4307)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [192][140/350]\tBatch Time 0.620 (0.620)\tData Load Time 0.000 (0.001)\tLoss 0.3980 (0.4287)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [192][210/350]\tBatch Time 0.596 (0.618)\tData Load Time 0.000 (0.000)\tLoss 0.3813 (0.4295)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [192][280/350]\tBatch Time 0.614 (0.619)\tData Load Time 0.000 (0.000)\tLoss 0.4879 (0.4265)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.320 (0.320)\tLoss 0.4480 (0.4480)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.248 (0.259)\tLoss 0.4164 (0.4116)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 12\n",
            "\n",
            "Epoch: [193][0/350]\tBatch Time 0.716 (0.716)\tData Load Time 0.069 (0.069)\tLoss 0.4814 (0.4814)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [193][70/350]\tBatch Time 0.689 (0.625)\tData Load Time 0.000 (0.001)\tLoss 0.3721 (0.4229)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [193][140/350]\tBatch Time 0.583 (0.626)\tData Load Time 0.000 (0.001)\tLoss 0.3877 (0.4270)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [193][210/350]\tBatch Time 0.616 (0.623)\tData Load Time 0.000 (0.000)\tLoss 0.4061 (0.4278)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [193][280/350]\tBatch Time 0.603 (0.622)\tData Load Time 0.000 (0.000)\tLoss 0.5914 (0.4279)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.320 (0.320)\tLoss 0.4343 (0.4343)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.255 (0.253)\tLoss 0.5021 (0.4169)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.420, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9986\n",
            "\n",
            "\n",
            "Epochs since last improvement: 13\n",
            "\n",
            "Epoch: [194][0/350]\tBatch Time 0.667 (0.667)\tData Load Time 0.055 (0.055)\tLoss 0.3876 (0.3876)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [194][70/350]\tBatch Time 0.631 (0.629)\tData Load Time 0.000 (0.001)\tLoss 0.4954 (0.4263)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [194][140/350]\tBatch Time 0.571 (0.622)\tData Load Time 0.000 (0.001)\tLoss 0.3908 (0.4293)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [194][210/350]\tBatch Time 0.627 (0.619)\tData Load Time 0.000 (0.000)\tLoss 0.3750 (0.4297)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [194][280/350]\tBatch Time 0.590 (0.618)\tData Load Time 0.000 (0.000)\tLoss 0.4495 (0.4289)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.314 (0.314)\tLoss 0.4924 (0.4924)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.256 (0.255)\tLoss 0.3630 (0.4181)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.421, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9988\n",
            "\n",
            "\n",
            "Epochs since last improvement: 14\n",
            "\n",
            "Epoch: [195][0/350]\tBatch Time 0.824 (0.824)\tData Load Time 0.056 (0.056)\tLoss 0.3751 (0.3751)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [195][70/350]\tBatch Time 0.601 (0.619)\tData Load Time 0.000 (0.001)\tLoss 0.4680 (0.4285)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [195][140/350]\tBatch Time 0.599 (0.612)\tData Load Time 0.000 (0.001)\tLoss 0.4188 (0.4328)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [195][210/350]\tBatch Time 0.583 (0.612)\tData Load Time 0.000 (0.000)\tLoss 0.3939 (0.4272)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [195][280/350]\tBatch Time 0.717 (0.616)\tData Load Time 0.000 (0.000)\tLoss 0.3459 (0.4272)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.334 (0.334)\tLoss 0.4270 (0.4270)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.255 (0.268)\tLoss 0.3658 (0.4268)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.423, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9979\n",
            "\n",
            "\n",
            "Epochs since last improvement: 15\n",
            "\n",
            "Epoch: [196][0/350]\tBatch Time 0.720 (0.720)\tData Load Time 0.061 (0.061)\tLoss 0.4264 (0.4264)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [196][70/350]\tBatch Time 0.612 (0.625)\tData Load Time 0.000 (0.001)\tLoss 0.4783 (0.4214)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [196][140/350]\tBatch Time 0.598 (0.623)\tData Load Time 0.000 (0.001)\tLoss 0.4955 (0.4217)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [196][210/350]\tBatch Time 0.734 (0.621)\tData Load Time 0.000 (0.000)\tLoss 0.4904 (0.4233)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [196][280/350]\tBatch Time 0.597 (0.616)\tData Load Time 0.000 (0.000)\tLoss 0.4725 (0.4261)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.324 (0.324)\tLoss 0.4501 (0.4501)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.280 (0.256)\tLoss 0.3943 (0.4214)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.421, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9983\n",
            "\n",
            "\n",
            "Epochs since last improvement: 16\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000007\n",
            "\n",
            "Epoch: [197][0/350]\tBatch Time 0.801 (0.801)\tData Load Time 0.075 (0.075)\tLoss 0.4112 (0.4112)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [197][70/350]\tBatch Time 0.607 (0.633)\tData Load Time 0.000 (0.001)\tLoss 0.5225 (0.4260)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [197][140/350]\tBatch Time 0.595 (0.629)\tData Load Time 0.000 (0.001)\tLoss 0.5849 (0.4295)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [197][210/350]\tBatch Time 0.611 (0.621)\tData Load Time 0.000 (0.001)\tLoss 0.3751 (0.4273)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [197][280/350]\tBatch Time 0.592 (0.620)\tData Load Time 0.000 (0.000)\tLoss 0.3657 (0.4252)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.361 (0.361)\tLoss 0.3593 (0.3593)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.245 (0.258)\tLoss 0.4028 (0.4344)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.425, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9976\n",
            "\n",
            "\n",
            "Epochs since last improvement: 17\n",
            "\n",
            "Epoch: [198][0/350]\tBatch Time 0.952 (0.952)\tData Load Time 0.067 (0.067)\tLoss 0.3749 (0.3749)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [198][70/350]\tBatch Time 0.622 (0.621)\tData Load Time 0.000 (0.001)\tLoss 0.3707 (0.4243)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [198][140/350]\tBatch Time 0.583 (0.614)\tData Load Time 0.000 (0.001)\tLoss 0.4005 (0.4253)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [198][210/350]\tBatch Time 0.618 (0.617)\tData Load Time 0.000 (0.000)\tLoss 0.4356 (0.4250)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Epoch: [198][280/350]\tBatch Time 0.616 (0.618)\tData Load Time 0.000 (0.000)\tLoss 0.5081 (0.4278)\tTop-5 Accuracy 100.000 (100.000)\n",
            "Validation: [0/44]\tBatch Time 0.330 (0.330)\tLoss 0.4081 (0.4081)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "Validation: [22/44]\tBatch Time 0.243 (0.258)\tLoss 0.3845 (0.4200)\tTop-5 Accuracy 100.000 (100.000)\t\n",
            "\n",
            " * LOSS - 0.419, TOP-5 ACCURACY - 100.000, BLEU-4 - 0.9981\n",
            "\n",
            "\n",
            "Epochs since last improvement: 18\n",
            "\n",
            "Epoch: [199][0/350]\tBatch Time 0.956 (0.956)\tData Load Time 0.072 (0.072)\tLoss 0.3533 (0.3533)\tTop-5 Accuracy 100.000 (100.000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}